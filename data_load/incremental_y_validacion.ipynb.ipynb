{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ls0IDKfxxLo"
      },
      "outputs": [],
      "source": [
        "!pip install pymysql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F1xqodA9x2HK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import sqlalchemy\n",
        "from sqlalchemy.types import Integer, VARCHAR, Float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CgRE4qI5vbS"
      },
      "source": [
        "#### definiendo funciones de validacion de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "19y8sCizyG9G"
      },
      "outputs": [],
      "source": [
        "# def business_validator(folder):\n",
        "#   def search_file(file):\n",
        "#     import os\n",
        "#     file_list = os.listdir('/content/drive/MyDrive/test/Dataset Yelp/')\n",
        "#     for i in file_list:\n",
        "#       if 'business' in i:\n",
        "#         path = (f'/content/drive/MyDrive/test/Dataset Yelp/{i}')\n",
        "#     return path\n",
        "    \n",
        "#   path = search_file('business')\n",
        "#   def format_validation(path):\n",
        "#     import pathlib\n",
        "#     if pathlib.Path(path).suffix == '.json':\n",
        "#       return True\n",
        "#     else:\n",
        "#       return False\n",
        "\n",
        "#   def name_col_val(df):\n",
        "#     if df.columns.tolist() == ['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
        "#         'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n",
        "#         'attributes', 'categories', 'hours']:\n",
        "#       return True\n",
        "#     else: return False\n",
        "\n",
        "#   def n_col_val(df):\n",
        "#     if df.shape[1] == 14:\n",
        "#       return True\n",
        "#     else: return False\n",
        "    \n",
        "#   df = pd.read_json(path, lines=True, nrows=10)\n",
        "#   if format_validation(path) == True and name_col_val(df) == True and n_col_val(df) == True:\n",
        "#     print('business validado')\n",
        "#   else:\n",
        "#     print('business no validado')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "hLxepd1ear9j"
      },
      "outputs": [],
      "source": [
        "def users_validator(folder):\n",
        "\n",
        "  def search_file(file):\n",
        "    import os\n",
        "    file_list = os.listdir(folder)\n",
        "    for i in file_list:\n",
        "      if 'user' in i:\n",
        "        path = (folder+i)\n",
        "    return path\n",
        "\n",
        "  path = search_file('user')\n",
        "\n",
        "  def format_validation(path):\n",
        "    import pathlib\n",
        "    if pathlib.Path(path).suffix == '.json':\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  def name_col_val(df):\n",
        "    if df.columns.tolist() == ['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny',\n",
        "       'cool', 'elite', 'friends', 'fans', 'average_stars', 'compliment_hot',\n",
        "       'compliment_more', 'compliment_profile', 'compliment_cute',\n",
        "       'compliment_list', 'compliment_note', 'compliment_plain',\n",
        "       'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
        "       'compliment_photos']:\n",
        "      return True\n",
        "    else: return False\n",
        "\n",
        "  def n_col_val(df):\n",
        "    if df.shape[1] == 22:\n",
        "      return True\n",
        "    else: return False\n",
        "    \n",
        "  df = pd.read_json(path, lines=True, nrows=10)\n",
        "  if format_validation(path) == True and name_col_val(df) == True and n_col_val(df) == True:\n",
        "    return 'users validado'\n",
        "  else:\n",
        "    return'users no validado'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "CTiBUHG8gRAZ"
      },
      "outputs": [],
      "source": [
        "def reviews_validator(folder):\n",
        "\n",
        "  def search_file(file):\n",
        "    import os\n",
        "    file_list = os.listdir(folder)\n",
        "    for i in file_list:\n",
        "      if 'review' in i:\n",
        "        path = (folder+i)\n",
        "    return path\n",
        "\n",
        "  path = search_file('review')\n",
        "\n",
        "  def format_validation(path):\n",
        "    import pathlib\n",
        "    if pathlib.Path(path).suffix == '.json':\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  def name_col_val(df):\n",
        "    if df.columns.tolist() == ['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny',\n",
        "       'cool', 'text', 'date']:\n",
        "      return True\n",
        "    else: return False\n",
        "\n",
        "  def n_col_val(df):\n",
        "    if df.shape[1] == 9:\n",
        "      return True\n",
        "    else: return False\n",
        "    \n",
        "  df = pd.read_json(path, lines=True, nrows=10)\n",
        "  if format_validation(path) == True and name_col_val(df) == True and n_col_val(df) == True:\n",
        "    return 'reviews validado'\n",
        "  else:\n",
        "    return 'reviews no validado'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CROggUj8Zk-n"
      },
      "source": [
        "### validando los archivos de carga incremental, cambiar el valor de folder, con la direccion de la carpeta contenedora\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-L1MLs6WOjU",
        "outputId": "9dbdebb8-8600-480e-fc07-b201ec61e39c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "business validado\n",
            "reviews validado\n",
            "reviews validado\n"
          ]
        }
      ],
      "source": [
        "folder = '/content/drive/MyDrive/test/Dataset Yelp/'\n",
        "business_validator(folder)\n",
        "users_validator(folder)\n",
        "reviews_validator(folder)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aUhnKaOnhOkM"
      },
      "source": [
        "#### funcion de etl business_incremental, pendiente de correcci칩n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd3-iTXJ70jX"
      },
      "outputs": [],
      "source": [
        "# def transform_business(df_business):\n",
        "\n",
        "#   # parametros para conectarnos al data warehouse\n",
        "#   engine = create_engine('mysql+pymysql://root:projectyelp2022@34.176.218.33/prueba')\n",
        "\n",
        "#   df_business = df_business.rename(columns={'business_id':'old_id'})\n",
        "#   df_business['city'] = df_business['city'].apply(lambda x: x.lower())\n",
        "#   df_business['city'] = df_business['city'].str.replace(',','')\n",
        "#   df_business['city'] = df_business['city'].str.replace('.','')\n",
        "#   df_business['city'] = df_business['city'].apply(lambda x: x.strip())\n",
        "#   df_business['city'] = df_business['city'].apply(lambda x: \" \".join(x.split()))\n",
        "#   df_business['city'] = df_business['city'].apply(lambda x: x.title())\n",
        "#   df_business['categories'] = df_business['categories'].fillna('0')\n",
        "#   cat_list = ['Restaurants', 'Hotels & Travel', 'Food', 'Nightlife', 'Active Life', 'Arts & Entertainment', 'Beauty & Spas']\n",
        "\n",
        "#   def validator(value):\n",
        "#     if value in cat_list:\n",
        "#       return 1000\n",
        "#     else:\n",
        "#       return 0\n",
        "\n",
        "#   def cat_to_list(value):\n",
        "#     value = value.split(', ')\n",
        "#     value.sort(key=validator, reverse=True)\n",
        "#     return ', '.join(value)\n",
        "  \n",
        "#   df_business['categories'] = df_business['categories'].apply(lambda x: cat_to_list(x))\n",
        "\n",
        "#   city_dict = {'Belle Chase': 'Belle Chasse', 'Abington Township': 'Abington','Ashland City': 'Ashland','Bellefontaine Neighbors': 'Bellefontaine',\n",
        "#                'Bellville': 'Belleville','Belleair Blf': 'Belleair Bluffs','Bethel Township': 'Bethel','Bensalem Pa': 'Bensalem',\n",
        "#                'Bensalem Township': 'Bensalem','Boise City': 'Boise','Burlington Township': 'Burlington',\"Carney'S Point\": 'Carneys Point',\n",
        "#                'Cedar Brook': 'Cedarbrook','/': '','Conshohoeken':'Conshohocken','Delran Township': 'Delran','Delran Twp': 'Delran',\n",
        "#                'Concord Township': 'Concord','Deptford Township': 'Deptford','Eastampton Township': 'Eastampton',' Township': '',\n",
        "#                'Fairview Hts': 'Fairview','-': '',' City': '',' Twp': '','Bch': 'Beach','Land O Lakes':\"Land O'Lakes\",\"Land O' Lakes\":\"Land O'Lakes\",\n",
        "#                'Mccordsville': 'Mc Cordsville','Metarie': 'Metairie','Mt Laurel Twp Nj': 'Mt Laurel','Sqaure': 'Square','O Fallon': \"O'Fallon\",\n",
        "#                \"O' Fallon\": \"O'Fallon\",'Phila': 'Philadelphia','Philadephia': 'Philadelphia','Philly': 'Philadelphia',\n",
        "#                'Redingtn Shor': 'Redington Shore','Redington Shores': 'Redington Shore','Riverview Fl': 'Riverview','Saintt': 'Saint',\n",
        "#                'Tierre': 'Tierra',\"Town 'N' Country\": 'Town & Country','Town And Country': 'Town & Country','Town N Country': 'Town & Country',\n",
        "#                'Tuscon': 'Tucson'\n",
        "#              }\n",
        "\n",
        "#   df_business = df_business.replace({'city': city_dict})\n",
        "#   df_restaurants = df_business[df_business['categories'].str.contains('Restaurant')]\n",
        "  \n",
        "  \n",
        "#   def cat_to_col(value):\n",
        "#     aux_dict = {}\n",
        "#     x = value.split(', ')\n",
        "#     for i in x:\n",
        "#       aux_dict[i] = 1\n",
        "#     return aux_dict\n",
        "\n",
        "#   def dict_to_columns(df_i, column):\n",
        "#     df_i[column] = df_i[column].fillna('{}')\n",
        "#     df_i.reset_index(inplace=True)\n",
        "#     df_o = df_i.join(pd.json_normalize(df_i.pop(column)))\n",
        "#     if 'level_0' in df_o.columns:\n",
        "#       df_o.drop(['level_0'], axis=1, inplace=True)\n",
        "\n",
        "#     return df_o\n",
        "    \n",
        "#   def attr_to_list(i):\n",
        "#     attributes_list = []\n",
        "#     try:\n",
        "#       for j in i:\n",
        "#         if i[j] == 'True' or i[j] == '1' or i[j] == \"u'free'\":\n",
        "#           attributes_list.append(j)\n",
        "#       return ', '.join(attributes_list)\n",
        "#     except:\n",
        "#       return ''\n",
        "  \n",
        " \n",
        "\n",
        "  \n",
        "#   df_restaurants['attributes'] = df_restaurants['attributes'].apply(lambda x: attr_to_list(x))\n",
        "#   df_business['attributes'] = df_business['attributes'].apply(lambda x: attr_to_list(x))\n",
        "#   df_business = df_business[df_business['categories'].str.contains('Restaurants|Hotels & Travel|Food|Nightlife|Active Life|Arts & Entertainment|Beauty & Spas')]\n",
        "#   df_business = df_business.reset_index(drop=True)\n",
        "\n",
        "#   business_index = pd.DataFrame()\n",
        "#   business_index['id'] = list(range(1, len(df_business) + 1, 1))\n",
        "#   business_index['business_id'] = df_business['old_id'].copy()\n",
        "\n",
        "#   df_business.drop(['old_id'], axis=1, inplace=True)\n",
        "#   new_col = list(range(1, len(df_business) +1, 1))\n",
        "#   df_business.insert(loc = 0, column = 'business_id', value = new_col)\n",
        "\n",
        "#  # creando el dataset de city_state\n",
        "  \n",
        "#   last_city_id = engine.connect().execute('select max(city_state_id) from business_city_state;').fetchall()[0][0]\n",
        "\n",
        "#   df_city_state = pd.DataFrame()\n",
        "#   df_city_state['city'] = df_business['city']\n",
        "#   df_city_state['state'] = df_business['state']\n",
        "#   df_city_state['aux'] = df_city_state['city'] +', '+  df_city_state['state']\n",
        "  \n",
        "#   old_city_state = engine.connect().execute('select concat(city,\", \", state) from business_city_state;').fetchall()\n",
        "\n",
        "#   new_city_state = []\n",
        "\n",
        "#   city_state_list = df_city_state['aux'].tolist()\n",
        "\n",
        "#   for i in city_state_list:\n",
        "#     try:\n",
        "#       old_city_state.index(i)\n",
        "#     except:\n",
        "#       new_city_state.append(i)\n",
        "  \n",
        "#   df_city_state2 = pd.DataFrame()\n",
        "#   df_city_state2['aux'] = new_city_state\n",
        "#   df_city_state2.insert(loc = 0, column = 'city_state_id', value = list(range(last_city_id+1, len(df_city_state2)+1+last_city_id))) \n",
        "#   df_city_state2['city'] = df_city_state2['aux'].apply(lambda x: x.split(', ')[0])\n",
        "#   df_city_state2['state'] = df_city_state2['aux'].apply(lambda x: x.split(', ')[-1])\n",
        "#   df_city_state2.drop(['aux'], axis=1, inplace=True)\n",
        "#   df_city_state2 = df_city_state2[['city_state_id', 'city', 'state']]\n",
        "#   df2 = pd.merge(df_business, df_city_state2, left_on=['city', 'state'], right_on=['city', 'state'], how='inner')\n",
        "\n",
        "#   df_business['city_state_id'] = df2['city_state_id']\n",
        "#   df_business.drop(['city'], axis=1, inplace=True)\n",
        "#   df_business.drop(['state'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "#   # creando el dataframe de horas\n",
        "#   df_hours = pd.DataFrame()\n",
        "#   df_hours['hours'] = df_business['hours'].copy()\n",
        "#   df_hours['aux'] = df_hours['hours'].astype('str')\n",
        "#   df_hours = dict_to_columns(df_hours, 'hours')\n",
        "#   df_hours = df_hours.fillna(0)\n",
        "#   df_hours.drop(['index'], axis=1, inplace=True)\n",
        "#   df_hours = df_hours.drop_duplicates()\n",
        "  \n",
        "#   last_hour_id = engine.connect().execute('select max(hours_id) from business_hours;').fetchall()[0][0]\n",
        "\n",
        "#   df_hours.insert(loc = 0, column = 'hours_id', value = list(range(last_hour_id+1, len(df_hours)+1+last_hour_id)))\n",
        "\n",
        "#   df_business['hours'] = df_business['hours'].astype('str')\n",
        "#   df3 = pd.merge(df_business, df_hours, left_on='hours', right_on='aux', how='inner')\n",
        "#   df3.drop(['hours','aux','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'], axis=1, inplace=True)\n",
        "#   df_business['hours_id'] = df3['hours_id']\n",
        "#   df_business.drop(['hours'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "#   # creando el dataframe de attributes\n",
        "\n",
        "#   df_business['attributes'] = df_business['attributes'].astype('str')\n",
        "\n",
        "#   df_attributes = pd.DataFrame()\n",
        "  \n",
        "#   attr_val = (pd.read_sql_table('business_attributes', engine.connect()))['attributes'].tolist()\n",
        "#   old_attributes = engine.connect().execute('select categories from business_categories;').fetchall()\n",
        "\n",
        "#   new_attr = []\n",
        "#   attributes_list = df_business['attributes'].tolist()\n",
        "\n",
        "#   for i in attributes_list:\n",
        "#     try:\n",
        "#       old_attributes.index(i)\n",
        "#     except:\n",
        "#       new_attr.append(i)\n",
        "  \n",
        "#   df_attributes['attributes'] = new_attr\n",
        "\n",
        "#   last_attr_id = engine.connect().execute('select max(attributes_id) from business_attributes;').fetchall()[0][0]\n",
        "\n",
        "#   df_attributes.insert(loc = 0, column = 'attributes_id', value = list(range(last_attr_id+1, len(df_attributes)+1+last_attr_id)))\n",
        "#   df5 = pd.merge(df_business, df_attributes, left_on=['attributes'], right_on=['attributes'], how='inner')\n",
        "#   df_business['attributes_id'] = df5['attributes_id']\n",
        "#   df_business.drop(['attributes'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "#   # creando el dataframe de categories\n",
        "\n",
        "#   df_categories = pd.DataFrame()\n",
        "\n",
        "#   last_cat_id = engine.connect().execute('select max(categories_id) from business_categories;').fetchall()[0][0]\n",
        "#   old_cat = engine.connect().execute('select categories from business_categories;').fetchall()\n",
        "  \n",
        "#   new_cat = []\n",
        "#   categories_list = df_business['categories'].tolist()\n",
        "#   for i in categories_list:\n",
        "#     try:\n",
        "#       old_cat.index(i)\n",
        "#     except:\n",
        "#       new_cat.append(i)\n",
        "\n",
        "#   df_categories['categories'] = new_cat\n",
        "#   df_categories.insert(loc = 0, column = 'categories_id', value = list(range(last_cat_id+1, len(df_categories)+1+last_cat_id)))\n",
        "\n",
        "#   df6 = pd.merge(df_business, df_categories, left_on='categories', right_on='categories', how='inner')\n",
        "#   df_business['categories_id'] = df6['categories_id']\n",
        "  \n",
        "#   df_hours.drop(['aux'], axis=1, inplace=True)\n",
        "#   hours_aux = df_hours.columns.tolist()[1:]\n",
        "#   df_business.drop('categories', axis=1, inplace=True)\n",
        "#   for i in hours_aux:\n",
        "#     df_hours[i] = df_hours[i].astype('str')\n",
        "#   old_business_id = engine.connect().execute('select max(business_id) from business;').fetchall()[0][0]\n",
        "#   df_business['business_id'] = list(range(old_business_id+1, len(df_business)+1+old_business_id))\n",
        "  \n",
        "#   df_business.to_sql('business', con=engine, index=False, if_exists='append')\n",
        "#   df_hours.to_sql('business_hours', con=engine, index=False, if_exists='append')\n",
        "#   df_attributes.to_sql('business_attributes', con=engine, index=False, if_exists='append')\n",
        "#   df_categories.to_sql('business_categories', con=engine, index=False, if_exists='append')\n",
        "#   df_city_state2.to_sql('business_city_state', con=engine, index=False, if_exists='append')\n",
        "\n",
        "#   engine.dispose()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw6R6xgImLkc"
      },
      "source": [
        "#### Ejecucion de la carga incremental de business"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDoT-6xdCvJD"
      },
      "outputs": [],
      "source": [
        "# aqui ingresar el dataframe business incremental\n",
        "# df = pd.read_json('example_dir/business_incremental.json', lines=True)\n",
        "# transform_business(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformation function of user table (JC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_columns = None # para mostrar todas las columnas de los dataframes\n",
        "import numpy as np\n",
        "#!pip install pymysql\n",
        "import pymysql\n",
        "import sqlalchemy as db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to transform incremental users table \n",
        "def etl_users(chunk):\n",
        "    col_to_drop=['name','elite','compliment_hot','compliment_more', 'compliment_profile', 'compliment_cute', 'compliment_list', 'compliment_note', 'compliment_plain',\n",
        "                'compliment_cool', 'compliment_funny', 'compliment_writer','compliment_photos'] # droppable columns list\n",
        "    data_types={'average_stars':np.float32 , 'fans':'uint16', 'review_count':'uint16', 'cool':'uint32', 'useful':'uint32', 'funny':'uint32'} # data types dict for some columns\n",
        "    \n",
        "    chunk = chunk.drop(columns=col_to_drop).astype(data_types) # eliminate useless columns and change data types\n",
        "    chunk.yelping_since = pd.to_datetime(chunk.yelping_since, format='%Y-%m-%d %H:%M:%S') # chande data type to yelping since\n",
        "    return chunk"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformation function of reviews table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#establecimiento de conexi칩n con la base de datos pi_1\n",
        "database_username='root' # Nombre de cliente en MySQL Workbench\n",
        "database_password='4488' # Contrase침a de MySQL Workbench\n",
        "database_ip='localhost' # localizacion del servidor\n",
        "database_name='yelp_pg' # Nombre de Base de datos a la que nos conectaremos\n",
        "engine=db.create_engine(f'mysql+pymysql://{database_username}:{database_password}@{database_ip}/{database_name}')\n",
        "conexion=engine.connect()\n",
        "meta = db.MetaData()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#funcion de transformaci칩n para pasar al chunk\n",
        "\n",
        "def review_etl(df):\n",
        "\n",
        "  rdb = conexion.execute(f\"select * from business_index\")\n",
        "  codes_b = pd.DataFrame(rdb.fetchall())\n",
        "  codes_b.columns = rdb.keys()\n",
        "\n",
        "  rdb = conexion.execute(f\"select * from users_ids\")\n",
        "  codes_u = pd.DataFrame(rdb.fetchall())\n",
        "  codes_u.columns = rdb.keys()\n",
        "\n",
        "  df_business_id = codes_b\n",
        "  df_user_id = codes_u\n",
        "  del codes_b\n",
        "  del codes_u\n",
        "\n",
        "  #df.drop(['text'], axis=1, inplace=True)\n",
        "  df_aux = pd.merge(df, df_business_id, left_on='business_id', right_on='business_id', how='inner')\n",
        "  df_aux.drop(['business_id'], axis=1, inplace=True)\n",
        "  df_aux = df_aux.rename(columns={\"id\": \"id_business\"})\n",
        "\n",
        "  df_aux2 = pd.merge(df_aux, df_user_id, left_on='user_id', right_on='user_id', how='inner')\n",
        "  df_aux2.drop(['user_id'], axis=1, inplace=True)\n",
        "  df_aux2.drop(['useful','funny','cool'], axis=1, inplace=True)\n",
        "  df_aux2 = df_aux2.rename(columns={\"n_user_id\": \"id_user\"})\n",
        "  \n",
        "  df_aux2.reset_index(drop=True)\n",
        "  \n",
        "  df_aux2['date']= pd.to_datetime(df_aux2['date'])\n",
        "  df_aux2['year'] = df_aux2['date'].dt.year\n",
        "  df_aux2['month'] = df_aux2['date'].dt.month\n",
        "  \n",
        "  return df_aux2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Duplicates validation and load to DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to call old_codes from DB\n",
        "def call_old_codes(table_name, dict):\n",
        "    \n",
        "    column_name = dict['column_name'][dict['table_name'].index(table_name)]\n",
        "    ids_table_name = dict['ids_table_name'][dict['table_name'].index(table_name)]\n",
        "\n",
        "    rdb = conexion.execute(f\"select {column_name} from {ids_table_name}\")\n",
        "    old_codes = pd.DataFrame(rdb.fetchall())\n",
        "    old_codes.columns = rdb.keys()\n",
        "    \n",
        "    old_codes[f'indicator_{table_name}']=True\n",
        "    return old_codes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# funtion to compare old codes with codes in new datasets\n",
        "def compare(new_data, old_codes, table_name, dict):\n",
        "    on= dict['column_name'][dict['table_name'].index(table_name)]\n",
        "    data_to_load = new_data.merge(old_codes, how='left', on=on)\n",
        "    data_to_load = data_to_load[data_to_load[f'indicator_{table_name}'].isnull()]\n",
        "    del new_data\n",
        "    return data_to_load "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to load non duplicated and properly identified rows and new alphanumeric codes\n",
        "def load(data_to_load, table_name , dict):\n",
        "    id_col_name = dict['column_name'][dict['table_name'].index(table_name)]\n",
        "    ids_table_name = dict['ids_table_name'][dict['table_name'].index(table_name)]\n",
        "    drop_col = dict['drop_col'][dict['table_name'].index(table_name)]\n",
        "\n",
        "    new_codes = data_to_load[id_col_name] # take new alphanumeric codes\n",
        "    new_codes.to_sql(ids_table_name, conexion, index=False , if_exists='append') # load new alphanumeric codes\n",
        "\n",
        "    data_to_load.drop(columns=drop_col, inplace=True) # drop unnecesary columns\n",
        "    data_to_load.to_sql( table_name, conexion, index=False, if_exists='append') # load new data\n",
        "\n",
        "    return table_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dictionarie with names of id's tables, name of alphanumeric columns and columns to drop before load to each of users, business and reviews tables\n",
        "dict = {'table_name':['users', 'business' , 'reviews'] ,\n",
        " 'ids_table_name':[ 'users_ids', 'business_index' , 'idreviews' ] ,\n",
        " 'column_name':['user_id', 'business_id', 'review_id'],\n",
        " 'drop_col' : [['user_id','indicator_users'],['business_id','indicator_business'],['review_id','indicator_reviews']]\n",
        " }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to get percentage of repeated rows\n",
        "def repeated_per(len_new_data, len_dtl):\n",
        "   return 100* (len_new_data - len_dtl) / len_new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder = 'DataSet_YELP/pruebas_incremental/'  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# validation and incremental load of users\n",
        "for i in range(8,10):\n",
        "  if users_validator(folder) == 'users validado':\n",
        "\n",
        "  # apply transformation to new dataset if exist \n",
        "    new_data = pd.read_json(folder+f'user_incremental_con_repeticiones{i}.json' , lines=True) # here change the archive name to read\n",
        "    new_data = etl_users(new_data)\n",
        "    print('lectura ok')  \n",
        "\n",
        "  # duplicates validation and load the new data and new alphanumeric codes to DB\n",
        "    len_new_data_u = len(new_data) # amount of rows in new dataset\n",
        "    old_codes = call_old_codes('users', dict) # call alphanumeric codes from DB\n",
        "    data_to_load = compare(new_data, old_codes, 'users', dict) # compare new dataset with old codes discard duplicated rows\n",
        "    len_dtl_u = len(data_to_load) # amount of rows of data to load\n",
        "    load(data_to_load, 'users' , dict)  # load new rows to DB\n",
        "\n",
        "  # incremental data report\n",
        "    print(f'Se encontraron {repeated_per(len_new_data_u, len_dtl_u)}% de datos preexistentes en los datos de users, se cargaron {len_dtl_u} filas nuevas de un total de {len_new_data_u} filas') \n",
        "\n",
        "\n",
        "# if business_validator(folder) == 'business validado':\n",
        "  \n",
        "#   len_new_data_b = len(new_data) # amount of rows in new dataset\n",
        "#   old_codes = call_old_codes('business', dict) # call alphanumeric codes from DB\n",
        "#   data_to_load = compare(new_data, old_codes) # compare new dataset with old codes discard duplicated rows\n",
        "#   len_dtl_b = len(data_to_load) # amount of rows of data to load\n",
        "#   load(data_to_load, 'business' , dict) # load new rows to DB\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# validation and incremental data for reviews\n",
        "for i in range(30,35):\n",
        "\n",
        "  if reviews_validator(folder) == 'reviews validado': \n",
        "\n",
        "  # apply transformation to new dataset if exist \n",
        "    new_data = pd.read_json(folder+f'review_incremental_con_repeticiones{i}.json' , lines=True) # here change the archive name to read\n",
        "    len_new_revs_raw= len(new_data)\n",
        "    new_data = review_etl(new_data)   \n",
        "    print('lectura ok') \n",
        "    \n",
        "    # validation of new data (duplicates)\n",
        "    len_new_data_r = len(new_data)  # amount of rows in new dataset\n",
        "    old_codes = call_old_codes('reviews', dict) # call alphanumeric codes from DB\n",
        "    data_to_load = compare(new_data, old_codes, 'reviews', dict) # compare new dataset with old codes discard duplicated rows\n",
        "    len_dtl_r = len(data_to_load)  # amount of rows of data to load\n",
        "\n",
        "    # old_codes_u = call_old_codes('users', dict) # call user's alphanumeric codes from DB\n",
        "    # old_codes_b = call_old_codes('business', dict) # call busines's alphanumeric codes from DB\n",
        "    # data_to_load = data_to_load.merge(old_codes_b , how='inner', on='business_id').merge(old_codes_u, how='inner', on='user_id') # filter new review rows by existing user_id and business_id\n",
        "    # len_dtl_filtered = len(data_to_load) # amount of filtered rows of data to load\n",
        "\n",
        "    load(data_to_load, 'reviews' , dict) # load new rows to DB   \n",
        "    print(f'Se encontraron {repeated_per(len_new_data_r, len_dtl_r)}% de datos preexistentes en los datos de reviews, hay {len_dtl_r} filas nuevas de un total de {len_new_data_r} filas')\n",
        "    print(f'{len_new_revs_raw - len_dtl_r} filas de reviews nuevas no se cargaron porque no tienen user_id o business_id conocido o no pertencen a la categoria estudiada, repita el proceso asegurandose de actualizar las tablas users o business con los codigos faltantes \\n')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conexion.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-CgRE4qI5vbS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('Proyecto-Final-YELP-Us21KoUz')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "5a2230d546b76f8d6d8920d479c809d384b49ecfda9fd3e60bed90ca330c4d6d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
